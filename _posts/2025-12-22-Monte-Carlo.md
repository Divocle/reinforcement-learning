---
layout: post
title: " 蒙特卡罗策略估值详解：状态价值估计、首次访问 vs 每次访问、增量平均实现"
date: 2025-12-22  # 新增：Jekyll必需的日期字段，确保文章被正确识别
---

> **摘要**：本文系统梳理强化学习中 **蒙特卡罗**（Monte Carlo, MC）方法的核心理论与实践细节。内容涵盖：（1）状态价值函数的定义与估计目标；（2）两种主流 MC 估计策略——“首次访问”与“每次访问”的原理、区别与收敛性；（3）高效更新技术——增量平均的完整数学推导。所有公式均附详细推导。

---

## 1. 状态价值函数：蒙特卡罗方法的目标
在马尔可夫决策过程（MDP）中，给定一个策略 $\pi(a|s)$，**状态价值函数** $v_\pi(s)$ 定义为：

$$
v_\pi(s) \triangleq \mathbb{E}_\pi \left[ G_t \mid S_t = s \right]
= \mathbb{E}_\pi \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \,\bigg|\, S_t = s \right] \tag{1}
$$

其中：
- $G_t$ 是从时刻 $t$ 开始的**折扣回报**（discounted return）
- $\gamma \in [0,1]$ 是折扣因子
- 期望 $\mathbb{E}_\pi[\cdot]$ 表示所有由策略 $\pi$ 生成的轨迹上的平均

> **目标**：在不知道环境模型（即未知转移概率 $P(s'|s,a)$ 和奖励函数 $R(s,a)$）的情况下，仅通过与环境交互生成的 episode 数据，估计 $v_\pi(s)$。

---

## 2. 蒙特卡罗方法的基本思想
蒙特卡罗方法基于一个简单但强大的统计原理：**用样本均值逼近期望值**。

根据大数定律（Law of Large Numbers）：

$$
\lim_{n \to \infty} \frac{1}{n} \sum_{i=1}^{n} G^{(i)}(s) = \mathbb{E}[G_t \mid S_t = s] = v_\pi(s) \tag{2}
$$

其中 $G^{(i)}(s)$ 是第 $i$ 次访问状态 $s$ 时观测到的回报。

---

## 3. 两种蒙特卡罗估计方法
### 3.1 首次访问蒙特卡罗（First-Visit MC）
#### 定义
> 在一个 episode 中，**仅当状态 $s$ 第一次出现时**，才将其对应的回报 $G_t$ 记录为一次有效样本。

#### 动机
- 避免同一 episode 内因循环导致的**相关样本偏差**
- 理论上更接近独立同分布（i.i.d.）假设

#### 更新规则
对每个 episode：
1. 生成轨迹：$S_0, A_0, R_1, S_1, \dots, S_T$
2. 计算每个 $t$ 的回报 $G_t = \sum_{k=t}^{T-1} \gamma^{k-t} R_{k+1}$
3. 维护一个集合 `visited`，记录本 episode 已访问的状态
4. 对每个 $t$：
   - 若 $S_t \notin \text{visited}$：
     - 将 $G_t$ 加入 $s = S_t$ 的回报列表
     - $V(s) = \text{average of all } G_t \text{ for } s$
     - 将 $S_t$ 加入 `visited`

#### 示例
考虑一个简单的 episodic 环境，状态空间为 $\{A, B, C, D\}$，其中：
- $D$ 是**终止状态**
- 智能体从 $A$ 开始
- 策略 $\pi$ 允许在某些状态下返回（例如在 $C$ 可以回到 $B$）

某一次 episode 的完整交互轨迹如下：

<center>

| 时间步 $t$ | 状态 $S_t$ | 动作 $A_t$ | 奖励 $R_{t+1}$ |
|:------------:|:-------------:|:-------------:|:------------------:|
| 0          | A           | →           | 2                |
| 1          | B           | →           | 0                |
| 2          | C           | ←           | 4                |
| 3          | B           | ↓           | -1               |
| 4          | D           | —           | 5                |

</center>

#### 步骤 1：计算每个时间步的回报 $G_t$
设折扣因子 $\gamma = 1$（为简化计算），则回报为从该时刻起所有未来奖励之和：
$$
G_t = \sum_{k=t}^{T-1} R_{k+1} \tag{3}
$$

其中 $T = 4$（最后一个状态索引）。

逐项计算（从后往前）：
- $G_3 = R_4 = 5$
- $G_2 = R_3 + G_3 = 4 + 5 = 9$
- $G_1 = R_2 + G_2 = 0 + 9 = 9$
- $G_0 = R_1 + G_1 = 2 + 9 = 11$

整理结果：

<center>

| $t$ | $S_t$ | $R_{t+1}$ | $G_t$ |
|:------:|:--------:|:------------:|:--------:|
| 0    | A      | 2          | 11     |
| 1    | B      | 0          | 9      |
| 2    | C      | 4          | 9      |
| 3    | B      | -1         | 5      |
| 4    | D      | 5          | —      |

</center>

#### 步骤 2：应用“首次访问”规则
按时间顺序遍历轨迹，并维护一个集合 `visited = {}`，记录本 episode 中已**首次访问过**的状态。

<center>

| $t$ | $S_t$ | 是否首次访问 | 操作 |
|:------:|:--------:|:----------------:|:------:|
| 0    | A      | 是（`visited` 为空） | 将 $G_0 = 11$ 加入 A 的回报列表；`visited = {A}` |
| 1    | B      | 是（B 不在 `visited`） | 将 $G_1 = 9$ 加入 B 的回报列表；`visited = {A, B}` |
| 2    | C      | 是（C 不在 `visited`） | 将 $G_2 = 9$ 加入 C 的回报列表；`visited = {A, B, C}` |
| 3    | B      | 否（B 已在 `visited`） | **忽略** $G_3 = 5$，不更新 B |
| 4    | D      | 是（但通常不估值终止状态） | 可选：若估值，则加入；一般跳过 |

</center>

**有效样本**：
- A: $G = 11$
- B: $G = 9$（仅第一次）
- C: $G = 9$

##### 步骤 3：更新状态价值估计（假设这是第 $n$ 次 episode）
假设此前已有若干 episode，当前各状态的统计如下（仅展示 B）：

<center>

| 状态 | 已收集的回报列表 | 当前 $V(B)$ |
|:------:|:------------------:|:--------------:|
| B    | [8, 7]           | $(8+7)/2 = 7.5$ |

</center>

本次 episode 为 B 新增一个有效回报：9

→ 更新后：
- 回报列表变为 `[8, 7, 9]`
- $V(B) = \dfrac{8 + 7 + 9}{3} = \dfrac{24}{3} = 8.0$

##### 为什么忽略第二次 B？
因为在一个 episode 内，状态 B 的两次出现**不是独立事件**：
- 第二次 B 的回报（5）受到第一次 B 行为的影响
- 若同时计入 $G=9$ 和 $G=5$，会**高估 B 被访问的频率**，并引入**自相关偏差**

#### 总结
- **轨迹**：A → B → C → B → D
- **回报**：$G_0=11,\ G_1=9,\ G_2=9,\ G_3=5$
- **首次访问有效样本**：A:11、B:9、C:9
- **B 的第二次出现**（$t=3$, $G=5$）被**明确忽略**

### 3.2 每次访问蒙特卡罗（Every-Visit MC）
#### 定义
> 在一个 episode 中，**每次访问状态 $s$**，都将其对应的 $G_t$ 视为一次独立样本。

#### 动机
- 充分利用所有可用数据
- 实现更简单，无需维护“是否首次”标记
- 在实践中通常收敛更快

#### 更新规则
对每个 episode：
1. 生成轨迹：$S_0, A_0, R_1, \dots, S_T$
2. 计算每个 $t$ 的回报 $G_t$
3. 对每个 $t = 0,1,\dots,T-1$：
   - 将 $G_t$ 直接加入 $s = S_t$ 的回报列表
   - $V(s) = \text{average of all collected } G_t \text{ for } s$

#### 示例（沿用上述轨迹）
对轨迹 A→B→C→B→D 的回报 $G_0=11, G_1=9, G_2=9, G_3=5$：
- A 被访问 1 次 → 样本：$[11]$ → $V(A)=11$
- B 被访问 2 次 → 样本：$[9,5]$ → $V(B)=(9+5)/2=7$
- C 被访问 1 次 → 样本：$[9]$ → $V(C)=9$

---

## 4. 增量平均：高效更新价值估计
### 4.1 问题动机
若存储所有回报 $G^{(1)}, G^{(2)}, \dots, G^{(n)}$ 再求平均，会带来**巨大内存开销**（尤其当 $n$ 很大时）。
我们需要一种**实时更新**的方法：仅用当前估计值和新样本，即可完成均值更新。

### 4.2 数学推导
设已有 $k$ 个样本，均值为：
$$
\mu_k = \frac{1}{k} \sum_{j=1}^{k} x_j \tag{4}
$$

加入第 $k+1$ 个样本 $x_{k+1}$，新均值 $\mu_{k+1}$ 推导如下：
$$
\begin{aligned}
\mu_{k+1} &= \frac{1}{k+1} \left( \sum_{j=1}^k x_j + x_{k+1} \right) \\
&= \frac{1}{k+1} \left( k\mu_k + x_{k+1} \right) \\
&= \mu_k + \frac{1}{k+1} \left( x_{k+1} - \mu_k \right)
\end{aligned} \tag{5}
$$

得到 **增量平均核心公式**：
$$
\mu_{k+1} = \mu_k + \alpha_{k+1} \left( x_{k+1} - \mu_k \right) \tag{6}
$$

其中 $\alpha_{k+1} = \frac{1}{k+1}$ 称为**步长因子**，随着样本数量增加而递减。

### 4.3 应用于 MC 估值
将增量平均公式映射到蒙特卡罗状态价值估计：
- $\mu_k \rightarrow V(s)$：状态 $s$ 的当前价值估计
- $x_{k+1} \rightarrow G_t$：新观测到的回报样本
- $k \rightarrow N(s)$：状态 $s$ 已被访问的次数

得到 **MC 增量更新规则**：

$$
V(s) \leftarrow V(s) + \frac{1}{N(s)+1} \left( G_t - V(s) \right) \tag{7}
$$

同时更新访问计数器：

$$
N(s) \leftarrow N(s) + 1 \tag{8}
$$

### 4.4 数值验证
以状态 B 的更新为例，初始 $V(B)=0, N(B)=0$，样本依次为 $8,4,7$：
<center>

| 访问次数 | 新样本 $G_t$ | $V(s)$ 更新过程 | 最终 $V(s)$ | $N(s)$ 更新后 |
|:----------:|:-------------:|:------------------:|:--------------:|:---------------:|
| 初始 | — | $V=0$ | 0 | 0 |
| 1 | 8 | $V = 0 + \frac{1}{1}(8-0) = 8$ | 8 | 1 |
| 2 | 4 | $V = 8 + \frac{1}{2}(4-8) = 6$ | 6 | 2 |
| 3 | 7 | $V = 6 + \frac{1}{3}(7-6) \approx 6.333$ | 6.333 | 3 |

</center>

直接平均验证：$\frac{8+4+7}{3} = \frac{19}{3} \approx 6.333$，与增量更新结果一致。

---

## 5. 总结
1. **蒙特卡罗策略估值**的核心是**用样本回报的均值逼近状态价值的期望**，无需环境模型。
2. **首次访问 MC** 保证样本独立性，**每次访问 MC** 充分利用数据，二者均收敛到真实价值 $v_\pi(s)$。
3. **增量平均**是 MC 估值的高效实现方式，无需存储历史样本，实时更新价值估计。